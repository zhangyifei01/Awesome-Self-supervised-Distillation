# Awesome-Self-supervised-Learning-of-Lightweight-Models
## 轻量级模型自监督学习

Note that this repository consists of **Distillation-based** methods (*aka. Self-supervised Distillation*) and **Non-distillation** methods.

**[ Updating ...... ]**



## 2022   


***
- Improving Self-Supervised Lightweight Model Learning via Hard-Aware Metric Distillation (**SMD** - <font color="#dd0000">**ECCV**22 Oral</font>) [[paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910286.pdf) [[code]](https://github.com/liuhao-lh/SMD)

    <font color=Gray><b>· · Author(s)</b>:</font> Hao Liu, Mang Ye <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Wuhan University; Beijing Institute of Technology <br>

***
- DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning (**DisCo** - <font color="#dd0000">**ECCV**22 Oral</font>) [[paper]](https://arxiv.org/abs/2104.09124) [[code]](https://github.com/Yuting-Gao/DisCo-pytorch)
    
    <font color=Gray><b>· · Author(s)</b>:</font> Yuting Gao, Jia-Xin Zhuang, Shaohui Lin, Hao Cheng, Xing Sun, Ke Li, Chunhua Shen <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Tencent Youtu Lab; Hong Kong University of Science and Technology; East China Normal University; Zhejiang University <br>

***
- Bag of Instances Aggregation Boosts Self-supervised Distillation (**BINGO** - <font color="#dd0000">**ICLR**22</font>) [[paper]](https://openreview.net/forum?id=N0uJGWDw21d) [[code]](https://github.com/haohang96/bingo)
   
    <font color=Gray><b>· · Author(s)</b>:</font> Haohang Xu, Jiemin Fang, XIAOPENG ZHANG, Lingxi Xie, Xinggang Wang, Wenrui Dai, Hongkai Xiong, Qi Tian <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Shanghai Jiao Tong University; Huawei Inc.; Huazhong University of Science & Technology <br>

***
- Representation Distillation by Prototypical Contrastive Predictive Coding (**ProtoCPC** - <font color="#dd0000">**ICLR**22</font>) [[paper]](https://openreview.net/forum?id=8la28hZOwug) 
   
    <font color=Gray><b>· · Author(s)</b>:</font> Kyungmin Lee <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Agency for Defense Development <br>
   

***
- Boosting Contrastive Learning with Relation Knowledge Distillation (**ReKD** - <font color="#dd0000">**AAAI**22</font>) [[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/20262) 
    
    <font color=Gray><b>· · Author(s)</b>:</font> Kai Zheng, Yuanjiang Wang, Ye Yuan <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Megvii Technology<br>

***
- On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals (**** - <font color="#dd0000">**AAAI**22</font>) [[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/20120)  [[code]](https://github.com/WOWNICE/ssl-small)
    
    <font color=Gray><b>· · Author(s)</b>:</font> Haizhou Shi, Youcai Zhang, Siliang Tang, Wenjie Zhu, Yaqian Li, Yandong Guo, Yueting Zhuang <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  OPPO Research Institute; Zhejiang University;  New York University <br>

***
- Attention Distillation: self-supervised vision transformer students need more guidance (**AttnDistill** - <font color="#dd0000">**BMVC**22</font>) [[paper]](https://arxiv.org/abs/2210.00944) [[code]](https://github.com/wangkai930418/attndistill)
    
    <font color=Gray><b>· · Author(s)</b>:</font> Kai Wang, Fei Yang, Joost van de Weijer <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Universitat Autònoma de Barcelona <br>

***
- Distilling Knowledge from Self-Supervised Teacher by Embedding Graph Alignment (**** - <font color="#dd0000">**BMVC**22</font>) [[paper not released]]()
    
    <font color=Gray><b>· · Author(s)</b>:</font> Yuchen Ma, Yanbei Chen, Zeynep Akata <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Heidelberg University; University of Tübingen <br>

***
- Dual-Level Knowledge Distillation via Knowledge Alignment and Correlation (**DLKD** - <font color="#dd0000">**TNNLS**22</font>) [[paper]](https://ieeexplore.ieee.org/document/9830618/)  [[code]](https://github.com/ifding/DLKD)
    
    <font color=Gray><b>· · Author(s)</b>:</font> Fei Ding, Yin Yang, Hongxin Hu, Venkat Krovi, Feng Luo <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Clemson University; University at Buffalo The State University of New York<br>
    
***
- Pixel-Wise Contrastive Distillation (**PCD** - <font color="#dd0000">**arXiv**22</font>) [[paper]](https://arxiv.org/abs/2211.00218) 
    
    <font color=Gray><b>· · Author(s)</b>:</font> Junqiang Huang, Zichao Guo <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Shopee <br>

***
- Effective Self-supervised Pre-training on Low-compute networks without Distillation (**** - <font color="#dd0000">**arXiv**22</font>) [[paper]](https://arxiv.org/abs/2210.02808) 
    
    <font color=Gray><b>· · Author(s)</b>:</font> Fuwen Tan, Fatemeh Saleh, Brais Martinez <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Samsung AI Cambridge; Microsoft Research Cambridge <br>

    
***
- A Closer Look at Self-supervised Lightweight Vision Transformers (**MAE-lite** - <font color="#dd0000">**arXiv**22</font>) [[paper]](https://arxiv.org/abs/2205.14443) 
    
    <font color=Gray><b>· · Author(s)</b>:</font> Shaoru Wang, Jin Gao, Zeming Li, Jian Sun, Weiming Hu <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>   Institute of Automation, Chinese Academy of Sciences; Megvii Technology; University of Chinese Academy of Sciences; CAS Center for Excellence in Brain Science and Intelligence Technology <br>
    

## 2021

***
- Distill on the Go: Online Knowledge Distillation in Self-Supervised Learning (**DoGo** - <font color="#dd0000">**CVPRW**21</font>) [[paper]](https://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Bhat_Distill_on_the_Go_Online_Knowledge_Distillation_in_Self-Supervised_Learning_CVPRW_2021_paper.html) 
    
    <font color=Gray><b>· · Author(s)</b>:</font> Prashant Bhat, Elahe Arani, Bahram Zonooz <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Advanced Research Lab, NavInfo Europe, Eindhoven, The Netherlands <br>
    

***
- Unsupervised Representation Transfer for Small Networks: I Believe I Can Distill On-the-Fly (**OSS** - <font color="#dd0000">**NeurIPS**21</font>) [[paper]](https://proceedings.neurips.cc/paper/2021/hash/cecd845e3577efdaaf24eea03af4c033-Abstract.html) 
    
    <font color=Gray><b>· · Author(s)</b>:</font> Hee Min Choi, Hyoa Kang, Dokwan Oh <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Samsung Advanced Institute of Technology <br>
    
***
- SEED: Self-supervised Distillation For Visual Representation  (**SEED** - <font color="#dd0000">**ICLR**21</font>) [[paper]](https://openreview.net/forum?id=AHm3dbp7D1D) 
   
    <font color=Gray><b>· · Author(s)</b>:</font> Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, Zicheng Liu <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Arizona State University; Microsoft Corporation <br>

***
- SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation  (**SimReg** - <font color="#dd0000">**BMVC**21</font>) [[paper]](https://www.bmvc2021-virtualconference.com/assets/papers/1137.pdf) [[code]](https://github.com/UCDvision/simreg)
   
    <font color=Gray><b>· · Author(s)</b>:</font> K. L. Navaneet, Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  University of Maryland; University of California, Davis <br>

***
- ProtoSEED: Prototypical Self-SupervisedRepresentation Distillation  (**ProtoSEED** - <font color="#dd0000">**NeurIPSW**21</font>) [[paper]](https://sslneurips21.github.io/pages/Accepted%20Paper.html) 
   
    <font color=Gray><b>· · Author(s)</b>:</font> Kyungmin Lee <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Agency for Defense Development <br>
    

***
- Simple Distillation Baselines for Improving Small Self-supervised Models  (**SimDis** - <font color="#dd0000">**arXiv**21</font>) [[paper]](https://openreview.net/forum?id=AHm3dbp7D1D) [[code]](https://github.com/JindongGu/SimDis/)
    
    <font color=Gray><b>· · Author(s)</b>:</font> Jindong Gu, Wei Liu, Yonglong Tian <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  University of Munich; Tencent; MIT <br>


***
- Self-Supervised Visual Representation Learning Using Lightweight Architectures  (**** - <font color="#dd0000">**arXiv**21</font>) [[paper]](https://arxiv.org/abs/2110.11160) 
    
    <font color=Gray><b>· · Author(s)</b>:</font> Prathamesh Sonawane, Sparsh Drolia, Saqib Shamsi, Bhargav Jain <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Pune Institute of Computer Technology; Whirlpool Corporation <br>
    

## 2020

***
- CompRess: Self-Supervised Learning by Compressing Representations (**CompRess** - <font color="#dd0000">**NeurIPS**20</font>) [[paper]](https://proceedings.neurips.cc/paper/2020/hash/975a1c8b9aee1c48d32e13ec30be7905-Abstract.html) [[code]](https://github.com/UMBCvision/CompRess)
    
    <font color=Gray><b>· · Author(s)</b>:</font> Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  University of Maryland <br>
    

## 2018

***
- Boosting Self-Supervised Learning via Knowledge Transfer ( **** - <font color="#dd0000">**CVPR**18</font>) [[paper]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Noroozi_Boosting_Self-Supervised_Learning_CVPR_2018_paper.pdf) 
    
    <font color=Gray><b>· · Author(s)</b>:</font> Mehdi Noroozi, Ananth Vinjimoor, Paolo Favaro, Hamed Pirsiavash <br>
    <font color=Gray><b>· · Organization(s)</b>:</font> University of Bern; University of Maryland, Baltimore County <br>
    


### **Some Related Influential Repositories**    </details>
***
- awesome-self-supervised-learning (star 5.2k)  [[link]](https://github.com/jason718/awesome-self-supervised-learning)

- Awesome-Knowledge-Distillation (star 2k) [[link]](https://github.com/FLHonker/Awesome-Knowledge-Distillation)

- DeepClustering (star 2k) [[link]](https://github.com/zhoushengisnoob/DeepClustering)


Thanks for the support of Prof. [Yu Zhou](https://people.ucas.ac.cn/~yuzhou).




