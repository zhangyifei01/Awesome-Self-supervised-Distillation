# Awesome-Self-supervised-Distillation
## 自监督蒸馏

**[ Updating ...... ]**



## 2022   


***
- Improving Self-Supervised Lightweight Model Learning via Hard-Aware Metric Distillation (**SMD** - <font color="#dd0000">**ECCV**22 Oral</font>) [[paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910286.pdf) [[code]](https://github.com/liuhao-lh/SMD)
    <details>
    <summary><font size=2>More Info</font></summary>
    <font color=Gray><b>· · Author(s)</b>:</font> Hao Liu, Mang Ye <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Wuhan University; Beijing Institute of Technology <br>
    <font color=Gray><b>· · Description</b>:</font>  <br> 
    <font color=Gray><b>· · Tags</b>:</font>  distillation
    </details>

***
- DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning (**DisCo** - <font color="#dd0000">**ECCV**22 Oral</font>) [[paper]](https://arxiv.org/abs/2104.09124) [[code]](https://github.com/Yuting-Gao/DisCo-pytorch)
    <details>
    <summary><font size=2>More Info</font></summary>
    <font color=Gray><b>· · Author(s)</b>:</font> Yuting Gao, Jia-Xin Zhuang, Shaohui Lin, Hao Cheng, Xing Sun, Ke Li, Chunhua Shen <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Tencent Youtu Lab; Hong Kong University of Science and Technology; East China Normal University; Zhejiang University <br>
    <font color=Gray><b>· · Description</b>:</font>  <br>
    <font color=Gray><b>· · Tags</b>:</font>  Self-supervised Distillation
    </details>

***
- Bag of Instances Aggregation Boosts Self-supervised Distillation (**BINGO** - <font color="#dd0000">**ICLR**22</font>) [[paper]](https://openreview.net/forum?id=N0uJGWDw21d) [[code]](https://github.com/haohang96/bingo)
    <details>
    <summary><font size=2>More Info</font></summary>
    <font color=Gray><b>· · Author(s)</b>:</font> Haohang Xu, Jiemin Fang, XIAOPENG ZHANG, Lingxi Xie, Xinggang Wang, Wenrui Dai, Hongkai Xiong, Qi Tian <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Shanghai Jiao Tong University; Huawei Inc.; Huazhong University of Science & Technology <br>
    <font color=Gray><b>· · Description</b>:</font>  <br>
    <font color=Gray><b>· · Tags</b>:</font>  Self-supervised distillation
    </details>

***
- Boosting Contrastive Learning with Relation Knowledge Distillation (**ReKD** - <font color="#dd0000">**AAAI**22</font>) [[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/20262) 
    <details>
    <summary><font size=2>More Info</font></summary>
    <font color=Gray><b>· · Author(s)</b>:</font> Kai Zheng, Yuanjiang Wang, Ye Yuan <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Megvii Technology<br>
    <font color=Gray><b>· · Description</b>:</font>  <br>
    <font color=Gray><b>· · Tags</b>:</font> distill
    </details>

## 2021

***
- Distill on the Go: Online Knowledge Distillation in Self-Supervised Learning (**DoGo** - <font color="#dd0000">**CVPRW**21</font>) [[paper]](https://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Bhat_Distill_on_the_Go_Online_Knowledge_Distillation_in_Self-Supervised_Learning_CVPRW_2021_paper.html) 
    <details>
    <summary><font size=2>More Info</font></summary>
    <font color=Gray><b>· · Author(s)</b>:</font> Prashant Bhat, Elahe Arani, Bahram Zonooz <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Advanced Research Lab, NavInfo Europe, Eindhoven, The Netherlands <br>
    <font color=Gray><b>· · Description</b>:</font>   <br>
    <font color=Gray><b>· · Tags</b>:</font>  distill
    </details>

***
- Unsupervised Representation Transfer for Small Networks: I Believe I Can Distill On-the-Fly (**OSS** - <font color="#dd0000">**NeurIPS**21</font>) [[paper]](https://proceedings.neurips.cc/paper/2021/hash/cecd845e3577efdaaf24eea03af4c033-Abstract.html) 
    <details>
    <summary><font size=2>More Info</font></summary>
    <font color=Gray><b>· · Author(s)</b>:</font> Hee Min Choi, Hyoa Kang, Dokwan Oh <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Samsung Advanced Institute of Technology <br>
    <font color=Gray><b>· · Description</b>:</font>   <br>
    <font color=Gray><b>· · Tags</b>:</font>  distill
    </details>
***
- SEED: Self-supervised Distillation For Visual Representation  (**SEED** - <font color="#dd0000">**ICLR**21</font>) [[paper]](https://openreview.net/forum?id=AHm3dbp7D1D) 
    <details>
    <summary><font size=2>More Info</font></summary>
    <font color=Gray><b>· · Author(s)</b>:</font> Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, Zicheng Liu <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  Arizona State University; Microsoft Corporation <br>
    <font color=Gray><b>· · Description</b>:</font>   <br>
    <font color=Gray><b>· · Tags</b>:</font>  Self-supervised distillation
    </details>

## 2020

***
- CompRess: Self-Supervised Learning by Compressing Representations (**CompRess** - <font color="#dd0000">**NeurIPS**20</font>) [[paper]](https://proceedings.neurips.cc/paper/2020/hash/975a1c8b9aee1c48d32e13ec30be7905-Abstract.html) [[code]](https://github.com/UMBCvision/CompRess)
    <details>
    <summary><font size=2>More Info</font></summary>
    <font color=Gray><b>· · Author(s)</b>:</font> Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash <br>
    <font color=Gray><b>· · Organization(s)</b>:</font>  University of Maryland <br>
    <font color=Gray><b>· · Description</b>:</font>  <br>
    <font color=Gray><b>· · Tags</b>:</font> self-supervised distillation
    </details>

## 2018

***
- Boosting Self-Supervised Learning via Knowledge Transfer ( **** - <font color="#dd0000">**CVPR**18</font>) [[paper]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Noroozi_Boosting_Self-Supervised_Learning_CVPR_2018_paper.pdf) [[~~No code~~]]() 
    <details>
    <summary><font size=2>More Info</font></summary>
    <font color=Gray><b>· · Author(s)</b>:</font> Mehdi Noroozi, Ananth Vinjimoor, Paolo Favaro, Hamed Pirsiavash <br>
    <font color=Gray><b>· · Organization(s)</b>:</font> University of Bern; University of Maryland, Baltimore County <br>
    <font color=Gray><b>· · Description</b>:</font>   <br>
    <font color=Gray><b>· · Tags</b>:</font> Self-supervised Distillation
    </details>


### **Some Related Influential Repositories**    </details>
***
- awesome-self-supervised-learning (star 5.2k)  [[link]](https://github.com/jason718/awesome-self-supervised-learning)

- Awesome-Knowledge-Distillation (star 2k) [[link]](https://github.com/FLHonker/Awesome-Knowledge-Distillation)

- DeepClustering (star 2k) [[link]](https://github.com/zhoushengisnoob/DeepClustering)


Thanks for the support of Prof. [Yu Zhou](https://people.ucas.ac.cn/~yuzhou).




